{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Libraries like <b> Pandas </b> and <b> scikit-learn </b> are suitable for mid-size data processing. Machine learning projects always deal with big data sizes that can not fit into one computer memory. The solution for big data processing is distributing its computation among different computing machine. Each machine will have a running code on subsets of data items and the results will be aggregated at the end in order to deliver a complete solution. <br>\n",
    "<br>\n",
    "<b> PySpark </b> is a python API that can handle the parallelization of data processing and provide an easy way to manipulate different issues related to distributing data, code, and collecting results.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, you need to initialized a <b> SparkContext </b> in order to establish a connection with clusters and run any operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You will need to create <b> SQLContext </b> in order to connect the Spark engine with different data sources and allow the Spark SQL commands over these data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After setup our working environment, it is time to specify your working data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. It is our data exploration time: Read a CSV file, and tell the Spark to automatically determine the data type by set <b> inferSchema = true </b> and print it using  <b> printSchema </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(SparkFiles.get(\"adult_data.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------+----------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|x  |age|workclass|fnlwgt|education |educational-num|marital-status    |occupation       |relationship|race |gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+---+---------+------+----------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|1  |25 |Private  |226802|11th      |7              |Never-married     |Machine-op-inspct|Own-child   |Black|Male  |0           |0           |40            |United-States |<=50K |\n",
      "|2  |38 |Private  |89814 |HS-grad   |9              |Married-civ-spouse|Farming-fishing  |Husband     |White|Male  |0           |0           |50            |United-States |<=50K |\n",
      "|3  |28 |Local-gov|336951|Assoc-acdm|12             |Married-civ-spouse|Protective-serv  |Husband     |White|Male  |0           |0           |40            |United-States |>50K  |\n",
      "+---+---+---------+------+----------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In order to ajust some variables data types (i.e. converting integer to float). We will create a function called <b> convertToFloat </b> and pass the columns names to it. We will use <b> withColumn </b> to inform the Spark which columns need a transformation into a float data type. The columns names are: <b> age, fnlwgt, capital-gain, educational-num , capital-loss, hours-per-week </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def convertToFloat(df, col_names, newType):\n",
    "    for name in col_names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: float (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: float (nullable = true)\n",
      " |-- capital-loss: float (nullable = true)\n",
      " |-- hours-per-week: float (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names  = ['age','fnlwgt','capital-gain', 'educational-num', 'capital-loss', 'hours-per-week']\n",
    "df = convertToFloat(df, col_names , FloatType())\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. We will explore our data step further in order to gain some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|  fnlwgt|\n",
      "+----+--------+\n",
      "|25.0|226802.0|\n",
      "|38.0| 89814.0|\n",
      "|28.0|336951.0|\n",
      "|44.0|160323.0|\n",
      "|18.0|103497.0|\n",
      "|34.0|198693.0|\n",
      "|29.0|227026.0|\n",
      "|63.0|104626.0|\n",
      "|24.0|369667.0|\n",
      "|55.0|104996.0|\n",
      "+----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','fnlwgt').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   education|count|\n",
      "+------------+-----+\n",
      "|   Preschool|   83|\n",
      "|     1st-4th|  247|\n",
      "|     5th-6th|  509|\n",
      "|   Doctorate|  594|\n",
      "|        12th|  657|\n",
      "|         9th|  756|\n",
      "| Prof-school|  834|\n",
      "|     7th-8th|  955|\n",
      "|        10th| 1389|\n",
      "|  Assoc-acdm| 1601|\n",
      "|        11th| 1812|\n",
      "|   Assoc-voc| 2061|\n",
      "|     Masters| 2657|\n",
      "|   Bachelors| 8025|\n",
      "|Some-college|10878|\n",
      "|     HS-grad|15784|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3606"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.age > 60).count()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " 'age',\n",
       " 'workclass',\n",
       " 'fnlwgt',\n",
       " 'education',\n",
       " 'marital-status',\n",
       " 'occupation',\n",
       " 'relationship',\n",
       " 'race',\n",
       " 'gender',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week',\n",
       " 'native-country',\n",
       " 'income']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('educational-num').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|      marital-status| avg(capital-gain)|\n",
      "+--------------------+------------------+\n",
      "|           Separated| 581.8424836601307|\n",
      "|       Never-married|  384.382639449029|\n",
      "|Married-spouse-ab...| 629.0047770700637|\n",
      "|            Divorced| 793.6755615860094|\n",
      "|             Widowed| 603.6442687747035|\n",
      "|   Married-AF-spouse|2971.6216216216217|\n",
      "|  Married-civ-spouse|1739.7006121810625|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('marital-status').agg({'capital-gain': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+\n",
      "|age_income|<=50K|>50K|\n",
      "+----------+-----+----+\n",
      "|      17.0|  595|   0|\n",
      "|      18.0|  862|   0|\n",
      "|      19.0| 1050|   3|\n",
      "|      20.0| 1112|   1|\n",
      "|      21.0| 1090|   6|\n",
      "|      22.0| 1161|  17|\n",
      "|      23.0| 1307|  22|\n",
      "|      24.0| 1162|  44|\n",
      "|      25.0| 1119|  76|\n",
      "|      26.0| 1068|  85|\n",
      "|      27.0| 1117| 115|\n",
      "|      28.0| 1101| 179|\n",
      "|      29.0| 1025| 198|\n",
      "|      30.0| 1031| 247|\n",
      "|      31.0| 1050| 275|\n",
      "|      32.0|  957| 296|\n",
      "|      33.0| 1045| 290|\n",
      "|      34.0|  949| 354|\n",
      "|      35.0|  997| 340|\n",
      "|      36.0|  948| 400|\n",
      "+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.crosstab('age', 'income').sort(\"age_income\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The first data pre-processing step is compute the square of age feature and add it as a new cloumn in our data set. From the above <b> age-income table </b>, we saw that age and income variables has a non-linear relationship. Young people has low income compared to the mid-age people. Also, the retired people has a fixed retirement low income. We will squared the ages and add its values in a new column called  <b> age_squared </b> in order to capture this non-linearity feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: float (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: float (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: float (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: float (nullable = true)\n",
      " |-- capital-loss: float (nullable = true)\n",
      " |-- hours-per-week: float (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      " |-- age_squared: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"age_squared\", df[\"age\"]**2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Removing a single observation is another pre-processing step since it has no added value to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native-country|count(native-country)|\n",
      "+--------------------+---------------------+\n",
      "|  Holand-Netherlands|                    1|\n",
      "|             Hungary|                   19|\n",
      "|            Honduras|                   20|\n",
      "|            Scotland|                   21|\n",
      "|Outlying-US(Guam-...|                   23|\n",
      "|                Laos|                   23|\n",
      "|          Yugoslavia|                   23|\n",
      "|     Trinadad&Tobago|                   27|\n",
      "|            Cambodia|                   28|\n",
      "|            Thailand|                   30|\n",
      "|                Hong|                   30|\n",
      "|             Ireland|                   37|\n",
      "|              France|                   38|\n",
      "|             Ecuador|                   45|\n",
      "|                Peru|                   46|\n",
      "|           Nicaragua|                   49|\n",
      "|              Greece|                   49|\n",
      "|                Iran|                   59|\n",
      "|              Taiwan|                   65|\n",
      "|            Portugal|                   67|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('native-country').agg({'native-country': 'count'}).sort(\"count(native-country)\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(df[\"native-country\"] != 'Holand-Netherlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|      native-country|count(native-country)|\n",
      "+--------------------+---------------------+\n",
      "|             Hungary|                   19|\n",
      "|            Honduras|                   20|\n",
      "|            Scotland|                   21|\n",
      "|          Yugoslavia|                   23|\n",
      "|Outlying-US(Guam-...|                   23|\n",
      "|                Laos|                   23|\n",
      "|     Trinadad&Tobago|                   27|\n",
      "|            Cambodia|                   28|\n",
      "|            Thailand|                   30|\n",
      "|                Hong|                   30|\n",
      "|             Ireland|                   37|\n",
      "|              France|                   38|\n",
      "|             Ecuador|                   45|\n",
      "|                Peru|                   46|\n",
      "|              Greece|                   49|\n",
      "|           Nicaragua|                   49|\n",
      "|                Iran|                   59|\n",
      "|              Taiwan|                   65|\n",
      "|            Portugal|                   67|\n",
      "|               Haiti|                   75|\n",
      "+--------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.groupby('native-country').agg({'native-country': 'count'}).sort(\"count(native-country)\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. We will create a data processing pipeline using  <b> PySpark </b>. Your data will input to the pipline from one side to have additional data analysis and transformation and the resulted transformed data will be the output from the other side of the pipline tunnel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a pipline that has the following stages for each categorical feature (i.e. column) we would like to transform it:\n",
    "1. The <b> StringIndexer </b>  will used to encode the string values in the columns using their corresponding frequencies numerical indexes.\n",
    "2. The resulted numerical representation of a categorical column will be encoded using <b> OneHotEncoder </b> .\n",
    "3. Aggregate the transformations of the encoded columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['workclassclassVec',\n",
       " 'educationclassVec',\n",
       " 'marital-statusclassVec',\n",
       " 'occupationclassVec',\n",
       " 'relationshipclassVec',\n",
       " 'raceclassVec',\n",
       " 'genderclassVec',\n",
       " 'native-countryclassVec',\n",
       " 'age',\n",
       " 'fnlwgt',\n",
       " 'capital-gain',\n",
       " 'educational-num',\n",
       " 'capital-loss',\n",
       " 'hours-per-week']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator,StringIndexer, OneHotEncoder, VectorAssembler\n",
    "CATE_FEATURES = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# convert the income column Here in a standalone stage because it contains un accepted encoded string values <=50K\n",
    "\n",
    "label_stringIdx =  StringIndexer(inputCol=\"income\", outputCol=\"newincome\")\n",
    "stages += [label_stringIdx]\n",
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + col_names\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "assemblerInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df_filtered)\n",
    "model = pipelineModel.transform(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the content of a new data set including all transformed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=1, age=25.0, workclass='Private', fnlwgt=226802.0, education='11th', educational-num=7.0, marital-status='Never-married', occupation='Machine-op-inspct', relationship='Own-child', race='Black', gender='Male', capital-gain=0.0, capital-loss=0.0, hours-per-week=40.0, native-country='United-States', income='<=50K', age_squared=625.0, workclassIndex=0.0, workclassclassVec=SparseVector(8, {0: 1.0}), educationIndex=5.0, educationclassVec=SparseVector(15, {5: 1.0}), marital-statusIndex=1.0, marital-statusclassVec=SparseVector(6, {1: 1.0}), occupationIndex=6.0, occupationclassVec=SparseVector(14, {6: 1.0}), relationshipIndex=2.0, relationshipclassVec=SparseVector(5, {2: 1.0}), raceIndex=1.0, raceclassVec=SparseVector(4, {1: 1.0}), genderIndex=0.0, genderclassVec=SparseVector(1, {0: 1.0}), native-countryIndex=0.0, native-countryclassVec=SparseVector(40, {0: 1.0}), newincome=0.0, features=SparseVector(99, {0: 1.0, 13: 1.0, 24: 1.0, 35: 1.0, 45: 1.0, 49: 1.0, 52: 1.0, 53: 1.0, 93: 25.0, 94: 226802.0, 96: 7.0, 98: 40.0}))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created model will be converted into data frame in order to have faster computations. The <b> newincome </b> and <b> features </b> vector will be selected and passed to a map function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newincome\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|newincome|            features|\n",
      "+---------+--------------------+\n",
      "|      0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|      0.0|[1.0,0.0,0.0,0.0,...|\n",
      "|      1.0|[0.0,0.0,1.0,0.0,...|\n",
      "|      1.0|[1.0,0.0,0.0,0.0,...|\n",
      "|      0.0|[0.0,0.0,0.0,1.0,...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = sqlContext.createDataFrame(input_data, [\"newincome\", \"features\"])\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the income below/above 50k in Train/Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|newincome|count(newincome)|\n",
      "+---------+----------------+\n",
      "|      0.0|           29675|\n",
      "|      1.0|            9300|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupby('newincome').agg({'newincome': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|newincome|count(newincome)|\n",
      "+---------+----------------+\n",
      "|      0.0|            7479|\n",
      "|      1.0|            2387|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.groupby('newincome').agg({'newincome': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a LogisticRegression model using PySpark, the input features and the new income label will be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"newincome\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.0646612226124553,-0.15477705031513053,-0.05256419958240663,-0.16461515146237743,-0.13260631577277002,0.14400926626880758,0.18819650236987684,-0.2353917978203867,-0.19338304459729488,-0.06237284464140197,0.22197010817874666,0.3921501096221977,-0.009717254573177075,-0.30547416786715625,-0.01616078293549073,-0.33591160889667376,-0.43395132543138015,0.5259759783441236,-0.3726088476431483,-0.20031706298179744,0.5939988732725536,-0.3422531786300088,-0.39578598875147036,0.3302004575488265,-0.3455518167148451,-0.2176427962480631,-0.21092696634656583,-0.14178261082016475,-0.11691515241239045,0.1926352024673376,-0.06288041320729389,0.29293615805630757,-0.10537312673105333,0.03940507037834406,-0.2903719616465082,-0.21085151972080762,-0.16594107140569114,-0.13056531343398273,-0.287840247111254,-0.3277598776047411,0.1197259355737328,0.1154654388987926,-0.2700104536773988,0.2733255410052605,-0.19817569357300174,-0.2923832704155254,-0.2438164399638307,0.41678090501194914,-0.05612947361095976,-0.1848546473207692,-0.06139393863288108,-0.25772417076552717,0.16885556313038863,-0.10447301035716314,-0.3799299516416317,-0.19451013295372446,-0.04672166273757999,-0.08288615048934217,-0.23631465194321344,-0.05598880492784646,-0.2741174344279241,-0.15680065773731847,-0.18405724884491234,0.1046549796149934,-0.24710916003464425,-0.3881905453094405,-0.14068140189316442,0.11681776315385618,-0.43542342293730363,-0.10377292379938312,-0.27896089022374027,-0.14982748649632394,-0.3872290998222921,-0.6207512345082057,-0.18558883152065317,-0.07021003127647833,-0.14931382554848574,-0.04254852705563118,-0.034581977161956434,-0.3749060433864178,-0.45161858324843107,-0.34838010020120147,0.18473918358317656,0.07503541154214452,-0.4425677736407814,-0.29733280740340146,0.14929427077412583,-0.41112988600321093,0.22689366786806595,-0.505994712186013,-0.41195422471830645,-0.35077883706182167,-0.1830296394756972,0.006816248510976599,1.3460719372024098e-07,2.242911145254682e-05,0.028169227517533563,0.00022430176016130437,0.008927708045104203]\n",
      "Intercept: -2.06022152717871\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- newincome: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = linearModel.transform(test_data)\n",
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+\n",
      "|newincome|prediction|         probability|\n",
      "+---------+----------+--------------------+\n",
      "|      0.0|       0.0|[0.92131726922017...|\n",
      "|      0.0|       0.0|[0.93131679970636...|\n",
      "|      0.0|       0.0|[0.92519796563085...|\n",
      "|      0.0|       0.0|[0.92588868003451...|\n",
      "|      0.0|       0.0|[0.65092317616142...|\n",
      "|      0.0|       0.0|[0.66403478658096...|\n",
      "|      0.0|       0.0|[0.68682216312906...|\n",
      "|      0.0|       0.0|[0.73020427939745...|\n",
      "|      0.0|       0.0|[0.93244677609270...|\n",
      "|      0.0|       0.0|[0.84983961802768...|\n",
      "|      0.0|       0.0|[0.83923089166456...|\n",
      "|      0.0|       0.0|[0.82981292416418...|\n",
      "|      0.0|       0.0|[0.53568924540824...|\n",
      "|      0.0|       0.0|[0.64761644851679...|\n",
      "|      0.0|       0.0|[0.66413089901748...|\n",
      "|      0.0|       0.0|[0.57262721211016...|\n",
      "|      0.0|       0.0|[0.87546458359797...|\n",
      "|      0.0|       0.0|[0.80410590976764...|\n",
      "|      0.0|       0.0|[0.83910577058873...|\n",
      "|      0.0|       0.0|[0.51279486332586...|\n",
      "|      0.0|       0.0|[0.59330783138204...|\n",
      "|      0.0|       0.0|[0.57874388945317...|\n",
      "|      0.0|       0.0|[0.61774604982795...|\n",
      "|      0.0|       0.0|[0.72223979257027...|\n",
      "|      0.0|       1.0|[0.33869919576236...|\n",
      "|      0.0|       0.0|[0.74704956845597...|\n",
      "|      0.0|       0.0|[0.73256254428683...|\n",
      "|      0.0|       0.0|[0.79385214833832...|\n",
      "|      0.0|       0.0|[0.77416930920783...|\n",
      "|      0.0|       0.0|[0.75603726433141...|\n",
      "+---------+----------+--------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = predictions.select(\"newincome\", \"prediction\", \"probability\")\n",
    "selected.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|newincome|count(newincome)|\n",
      "+---------+----------------+\n",
      "|      0.0|            7479|\n",
      "|      1.0|            2387|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = predictions.select(\"newincome\", \"prediction\")\n",
    "cm.groupby('newincome').agg({'newincome': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|             8939|\n",
      "|       1.0|              927|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the model accuray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8216095682140685"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.filter(cm.newincome == cm.prediction).count() / cm.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> References: </b> <br>\n",
    "Since I am newbie with a Spark :D, I took this assignment as a first step towards learning PySpark for the first time ever :D. This link (https://www.guru99.com/pyspark-tutorial.html#10) was very useful and help me to understand many concepts. Also, I found some many coding errors there, so my notebook will be useful for others too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
